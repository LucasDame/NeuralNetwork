# Neural Network from Scratch in C

Un r√©seau de neurones enti√®rement impl√©ment√© en C pur, sans d√©pendances externes. Ce projet √©ducatif vise √† comprendre les fondamentaux du deep learning en construisant tout de z√©ro.

## üìã Table des mati√®res

- [Caract√©ristiques](#caract√©ristiques)
- [Structure du projet](#structure-du-projet)
- [Installation](#installation)
- [Utilisation](#utilisation)
- [Architecture](#architecture)
- [Exemple : XOR](#exemple--xor)
- [Test sur MNIST](#test-sur-mnist)
- [Math√©matiques](#math√©matiques)
- [Roadmap](#roadmap)
- [Contribuer](#contribuer)

## ‚ú® Caract√©ristiques

- **Pur C** : Aucune d√©pendance externe (pas de biblioth√®ques ML)
- **Architecture flexible** : Nombre de couches et neurones configurables
- **Fonctions d'activation** : ReLU et Sigmoid
- **Backpropagation** : Impl√©mentation compl√®te de la r√©tropropagation du gradient
- **Mini-batch training** : Support pour l'entra√Ænement par batch
- **Optimis√©** : Utilisation de matrices pour des calculs efficaces

## üìÅ Structure du projet

```
.
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ matrix.c        # Op√©rations matricielles (multiplication, transposition, etc.)
‚îÇ   ‚îú‚îÄ‚îÄ layer.c         # D√©finition et op√©rations sur une couche de neurones
‚îÇ   ‚îú‚îÄ‚îÄ network.c       # Gestion du r√©seau multicouche
‚îÇ   ‚îî‚îÄ‚îÄ main.c          # Point d'entr√©e et exemples
‚îú‚îÄ‚îÄ Makefile            # Compilation du projet
‚îî‚îÄ‚îÄ README.md           # Ce fichier
```

## üöÄ Installation

### Pr√©requis

- GCC (ou tout compilateur C compatible)
- Make
- Linux/macOS/WSL (non test√© sur Windows natif)

### Compilation

```bash
# Compiler le projet
make

# Ex√©cuter le programme
./neural_network
```

### Nettoyage

```bash
make clean
```

## üíª Utilisation

### Exemple basique (XOR)

Le fichier `main.c` contient actuellement un exemple d'entra√Ænement sur le probl√®me XOR :

```c
// Cr√©ation du r√©seau : 2 entr√©es -> 3 neurones cach√©s -> 1 sortie
int layers[] = {2, 3, 1};
Network net = create_network(layers, 3);

// Donn√©es d'entra√Ænement
// Input: (0,0) -> Output: 0
// Input: (0,1) -> Output: 1
// Input: (1,0) -> Output: 1
// Input: (1,1) -> Output: 0

// Entra√Ænement
int epochs = 10000;
double learning_rate = 0.1;
train_network(&net, inputs, targets, learning_rate, batch_size, batch_counter);
```

### R√©sultats attendus

Apr√®s 10 000 √©poques, le r√©seau devrait produire :

```
Resultats apres entrainement :
Entree: 0 0 -> Sortie: 0.02145 (Cible: 0)
Entree: 0 1 -> Sortie: 0.97823 (Cible: 1)
Entree: 1 0 -> Sortie: 0.98012 (Cible: 1)
Entree: 1 1 -> Sortie: 0.03421 (Cible: 0)
```

## üèóÔ∏è Architecture

### Composants principaux

#### 1. **Matrix** (`matrix.c`)
Gestion des op√©rations matricielles :
- `create_matrix()` : Allocation de matrices
- `multiply_matrices()` : Multiplication matricielle
- `transpose_matrix()` : Transposition
- `add_matrices()` : Addition
- `elementwise_multiply_matrix()` : Produit de Hadamard
- `substract_matrix()` : soustraction matricielle
- `scalar_multiply_matrix()` : Multiplication par un scalaire
- `reset_matrix()` : R√©initialisation √† z√©ro
- `free_matrix()` : Lib√©ration de la m√©moire
- `print_matrix()` : Affichage d'une matrice (pour le debug)
- `get_element()` : Acc√®s √† un √©l√©ment sp√©cifique
- `set_element()` : Modification d'un √©l√©ment sp√©cifique
- `copy_matrix()` : Copie d'une matrice
- `free_matrix()` : Lib√©ration de la m√©moire d'une matrice

#### 2. **Layer** (`layer.c`)
Repr√©sentation d'une couche de neurones :
```c
typedef struct {
    Matrix weights;     // Poids de la couche
    Matrix biases;      // Biais de la couche
    
    Matrix z;           // Stockera (Input * Weights + Biases)
    Matrix activation;  // Stockera f(z)

    ActivationFunc func;    // Fonction d'activation
    ActivationFunc deriv;   // D√©riv√©e de la fonction d'activation

    Matrix delta;            // Erreur locale (backprop)
    Matrix weight_gradients; // Gradients des poids
    Matrix bias_gradients;   // Gradients des biais
    
    Matrix t_weights;    // Transpos√©e des poids
    Matrix t_biases;     // Transpos√©e des biais
    Matrix error_temp;   // Buffer pour calculs interm√©diaires
    Matrix z_prime;      // D√©riv√©e de z
    Matrix t_input;      // Transpos√©e de l'entr√©e
    Matrix buffer;       // Buffer pour calculs interm√©diaires
} Layer;
```

**Op√©rations** :
- `forward_layer()` : Propagation avant (z = input √ó W + b, a = f(z))
- `compute_z_prime()` : Calcul de la d√©riv√©e de l'activation
- `apply_activation` : Application de la fonction d'activation
- `create_layer()` : Initialisation d'une couche
- `free_layer()` : Lib√©ration de la m√©moire d'une couche
- `relu()` : Fonction d'activation ReLU 
- `relu_derivative()` : D√©riv√©e de ReLU 
- `sigmoid()` : Fonction d'activation Sigmoid 
- `sigmoid_derivative()` : D√©riv√©e de Sigmoid

#### 3. **Network** (`network.c`)
Gestion du r√©seau complet :
```c
typedef struct {
    Layer* layers;
    int num_layers;
} Network;
```

**Op√©rations** :
- `forward_network()` : Propagation avant compl√®te
- `train_network()` : Entra√Ænement (forward + backward + update)
- `create_network()` : Initialisation du r√©seau
- `free_network()` : Lib√©ration de la m√©moire du r√©seau

### Fonctions d'activation

#### ReLU (Rectified Linear Unit)
```
f(x) = max(0, x)
f'(x) = 1 si x > 0, sinon 0
```

#### Sigmoid
```
f(x) = 1 / (1 + e^(-x))
f'(x) = f(x) √ó (1 - f(x))
```

## üìê Math√©matiques

### Forward Pass

Pour une couche $l$ :

$$z^{[l]} = a^{[l-1]} W^{[l]} + b^{[l]}$$

$$a^{[l]} = f(z^{[l]})$$

O√π :
- $a^{[l-1]}$ : activation de la couche pr√©c√©dente
- $W^{[l]}$ : matrice des poids
- $b^{[l]}$ : vecteur de biais
- $f$ : fonction d'activation

### Backward Pass

#### Derni√®re couche :
$$\delta^{[L]} = (a^{[L]} - y) \odot f'(z^{[L]})$$

#### Couches cach√©es :
$$\delta^{[l]} = (\delta^{[l+1]} W^{[l+1]T}) \odot f'(z^{[l]})$$

### Gradients :
$$\frac{\partial L}{\partial W^{[l]}} = (a^{[l-1]})^T \delta^{[l]}$$

$$\frac{\partial L}{\partial b^{[l]}} = \delta^{[l]}$$

### Mise √† jour (Gradient Descent) :
$$W^{[l]} := W^{[l]} - \alpha \frac{\partial L}{\partial W^{[l]}}$$

$$b^{[l]} := b^{[l]} - \alpha \frac{\partial L}{\partial b^{[l]}}$$

O√π $\alpha$ est le learning rate et $\odot$ d√©signe le produit de Hadamard (√©l√©ment par √©l√©ment).

## üó∫Ô∏è Roadmap

### ‚úÖ Impl√©ment√©
- [x] Op√©rations matricielles de base
- [x] Couches fully-connected
- [x] Fonctions d'activation (ReLU, Sigmoid)
- [x] Forward propagation
- [x] Backpropagation
- [x] Mini-batch training
- [x] Exemple XOR fonctionnel

### üöß En cours / √Ä venir
- [ ] Parser MNIST
- [ ] Fonction de perte (Cross-Entropy)
- [ ] M√©triques (accuracy, loss)
- [ ] Sauvegarde/chargement de mod√®les
- [ ] Optimiseurs (Adam, RMSprop)
- [ ] Dropout pour la r√©gularisation
- [ ] Batch Normalization
- [ ] Couches convolutionnelles (CNN)
- [ ] Interface CLI pour configurer le r√©seau
- [ ] Visualisation de l'apprentissage
- [ ] Support GPU (optionnel)

## üêõ Bugs connus

- **Initialisation** : L'initialisation des poids pourrait √™tre am√©lior√©e (Xavier/He initialization)
- **Overflow** : Pas de protection contre les valeurs num√©riques extr√™mes

## ü§ù Contribuer

Les contributions sont les bienvenues ! N'h√©sitez pas √† :

1. Fork le projet
2. Cr√©er une branche (`git checkout -b feature/AmazingFeature`)
3. Commit vos changements (`git commit -m 'Add AmazingFeature'`)
4. Push vers la branche (`git push origin feature/AmazingFeature`)
5. Ouvrir une Pull Request

### Id√©es de contributions
- Optimiser les performances
- Ajouter des tests unitaires
- Am√©liorer la documentation

## üìö Ressources

- [3Blue1Brown - Neural Networks](https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi)
- [MNIST Database](http://yann.lecun.com/exdb/mnist/)

---

‚≠ê Si ce projet vous a aid√©, n'h√©sitez pas √† mettre une √©toile !